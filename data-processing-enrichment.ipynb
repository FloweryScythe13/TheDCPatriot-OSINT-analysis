{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>The purpose of this notebook is to perform all necessary cleaning, (pre-)processing, and enrichment of data collected from external OSIF sources, such as TweetBeaver or the Youtube Data API. </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><strong>Tweet Processing: @PLNewsToday timeline</strong></h5>\n",
    "\n",
    "The columns in the file `PLnewstoday timeline.csv` are `Tweet author`,`Tweet ID`,`Date posted`,`Tweet text`,`URL`,`Retweets`,`Favorited`,`Source`. Notably in comparison to other files of tweets such as `thedcpatriot-lancaster-twitter1.json` from Twint, this timeline file (which I downloaded from TweetBeaver) does not contain columns for hashtags, user mentions, hyperlinks, tweet geo-tagged location, and several other useful properties of tweets. Let's extract those properties from the tweets in the timeline ourselves and save the output to a new file, which we will call `PLnewstoday timeline processed.csv`. \n",
    "\n",
    "In addition to the standard tweet properties of interest like mentions, URLs, and hashtags, there are also some aspects of these tweets that are of special value to us due to the nature of their source (a journalist reporting from a war zone, with several of their stories deemed false or deceptive by outside experts). As such, I will use a few APIs on these tweets for additional feature extraction: the GATE Journalist Safety Analyzer, the GATE Rumour veracity classifier, and the GATE TwitIE Named Entity Recognizer for Tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: twitter-text-python in c:\\users\\dell\\miniconda3\\lib\\site-packages (1.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\dell\\miniconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install twitter-text-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from ttp import ttp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tweet author                Tweet ID                     Date posted  \\\n",
      "0  PLnewstoday  ID 1522888466773721088  Sat May 07 10:37:59 +0000 2022   \n",
      "1  PLnewstoday  ID 1522851834431516672  Sat May 07 08:12:25 +0000 2022   \n",
      "2  PLnewstoday  ID 1522477719707172864  Fri May 06 07:25:49 +0000 2022   \n",
      "\n",
      "                                          Tweet text  \\\n",
      "0   RT @PLnewstoday: ‚ö°Ô∏èüì£Inside Azovstal Territory...   \n",
      "1   ‚ö°Ô∏èüì£Inside Azovstal Territory: First Western J...   \n",
      "2   ‚ö°Ô∏èüì£Ukraine Snipers Killed Civilians In Mariup...   \n",
      "\n",
      "                                                 URL  Retweets  Favorited  \\\n",
      "0  https://twitter.com/PLnewstoday/statuses/15228...       209          0   \n",
      "1  https://twitter.com/PLnewstoday/statuses/15228...       209        409   \n",
      "2  https://twitter.com/PLnewstoday/statuses/15224...       296        558   \n",
      "\n",
      "            Source  \n",
      "0  Twitter Web App  \n",
      "1  Twitter Web App  \n",
      "2  Twitter Web App  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/PLnewstoday timeline.csv\")\n",
    "print(df.head(3))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet ID'] = df['Tweet ID'].str.slice_replace(stop=3)\n",
    "df['Mentions'] = [[] for _ in range(df.shape[0])]\n",
    "df['Hashtags'] = [[] for _ in range(df.shape[0])]\n",
    "df['URLs'] = [[] for _ in range(df.shape[0])] #will have to be careful about this name - easy to mix up with the URL column from TweetBeaver\n",
    "df.rename(columns={\"Tweet text\": \"Text\", \"URL\": \"Link\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ttp.Parser()\n",
    "#df['Mentions'] = df.eval(parse_result.[0] if (parse_result:= parser.parse(df['Tweet text'])) else None\n",
    "df['parsed_results'] = df['Text'].apply(parser.parse)\n",
    "df['Mentions'] = df['parsed_results'].apply(lambda x: x.users)\n",
    "df['Hashtags'] = df['parsed_results'].apply(lambda y: y.tags)\n",
    "df['URLs'] = df['parsed_results'].apply(lambda z: z.urls)\n",
    "del df['parsed_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tweet author             Tweet ID                     Date posted  \\\n",
      "0  PLnewstoday  1522888466773721088  Sat May 07 10:37:59 +0000 2022   \n",
      "1  PLnewstoday  1522851834431516672  Sat May 07 08:12:25 +0000 2022   \n",
      "2  PLnewstoday  1522477719707172864  Fri May 06 07:25:49 +0000 2022   \n",
      "\n",
      "                                                Text  \\\n",
      "0   RT @PLnewstoday: ‚ö°Ô∏èüì£Inside Azovstal Territory...   \n",
      "1   ‚ö°Ô∏èüì£Inside Azovstal Territory: First Western J...   \n",
      "2   ‚ö°Ô∏èüì£Ukraine Snipers Killed Civilians In Mariup...   \n",
      "\n",
      "                                                Link  Retweets  Favorited  \\\n",
      "0  https://twitter.com/PLnewstoday/statuses/15228...       209          0   \n",
      "1  https://twitter.com/PLnewstoday/statuses/15228...       209        409   \n",
      "2  https://twitter.com/PLnewstoday/statuses/15224...       296        558   \n",
      "\n",
      "            Source       Mentions            Hashtags  \\\n",
      "0  Twitter Web App  [PLnewstoday]                  []   \n",
      "1  Twitter Web App             []  [RussiaUkraineWar]   \n",
      "2  Twitter Web App             []                  []   \n",
      "\n",
      "                        URLs  \n",
      "0          [https://t.co/Sh]  \n",
      "1  [https://t.co/ShE3eRNN98]  \n",
      "2  [https://t.co/yPN16UFQP6]  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Facebook Profile Processing</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy-stanza\n",
      "  Downloading spacy_stanza-1.0.2-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy-stanza) (3.3.0)\n",
      "Collecting stanza<1.5.0,>=1.2.0\n",
      "  Downloading stanza-1.4.0-py3-none-any.whl (574 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (3.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (1.21.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (4.61.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (21.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (2.4.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (2.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (1.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (2.27.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (0.9.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (1.0.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (0.6.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (8.0.15)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (0.7.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from spacy<4.0.0,>=3.0.0->spacy-stanza) (1.0.7)\n",
      "Requirement already satisfied: protobuf in c:\\users\\dell\\miniconda3\\lib\\site-packages (from stanza<1.5.0,>=1.2.0->spacy-stanza) (3.19.1)\n",
      "Collecting emoji\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting torch>=1.3.0\n",
      "  Downloading torch-1.11.0-cp39-cp39-win_amd64.whl (157.9 MB)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\miniconda3\\lib\\site-packages (from stanza<1.5.0,>=1.2.0->spacy-stanza) (1.16.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\dell\\miniconda3\\lib\\site-packages (from stanza<1.5.0,>=1.2.0->spacy-stanza) (4.13.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy-stanza) (3.0.6)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<4.0.0,>=3.0.0->spacy-stanza) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy-stanza) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-stanza) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-stanza) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-stanza) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-stanza) (2.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<4.0.0,>=3.0.0->spacy-stanza) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-stanza) (8.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy-stanza) (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\miniconda3\\lib\\site-packages (from transformers->stanza<1.5.0,>=1.2.0->spacy-stanza) (3.4.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from transformers->stanza<1.5.0,>=1.2.0->spacy-stanza) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from transformers->stanza<1.5.0,>=1.2.0->spacy-stanza) (2021.11.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from transformers->stanza<1.5.0,>=1.2.0->spacy-stanza) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\dell\\miniconda3\\lib\\site-packages (from transformers->stanza<1.5.0,>=1.2.0->spacy-stanza) (0.2.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\dell\\miniconda3\\lib\\site-packages (from transformers->stanza<1.5.0,>=1.2.0->spacy-stanza) (0.0.46)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\miniconda3\\lib\\site-packages (from sacremoses->transformers->stanza<1.5.0,>=1.2.0->spacy-stanza) (1.1.0)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171030 sha256=8255b00b432a9d27ee08fe23b1498b450f21d019a575b87e6625e5e807f72187\n",
      "  Stored in directory: c:\\users\\dell\\appdata\\local\\pip\\cache\\wheels\\fa\\7a\\e9\\22dd0515e1bad255e51663ee513a2fa839c95934c5fc301090\n",
      "Successfully built emoji\n",
      "Installing collected packages: torch, emoji, stanza, spacy-stanza\n",
      "Successfully installed emoji-1.7.0 spacy-stanza-1.0.2 stanza-1.4.0 torch-1.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\dell\\miniconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy-entity-linker\n",
    "#!pip3 install geopy\n",
    "!pip3 install spacy-stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: 154kB [00:00, 21.8MB/s]                    \n",
      "2022-06-14 21:42:53 INFO: Downloading default packages for language: ar (Arabic)...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-ar/resolve/v1.4.0/models/default.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 397M/397M [02:49<00:00, 2.35MB/s] \n",
      "2022-06-14 21:45:49 INFO: Finished downloading models and saved to C:\\Users\\Dell\\stanza_resources.\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: 154kB [00:00, 17.1MB/s]                    \n",
      "2022-06-14 21:45:49 INFO: Downloading default packages for language: vi (Vietnamese)...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-vi/resolve/v1.4.0/models/default.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 385M/385M [03:04<00:00, 2.09MB/s] \n",
      "2022-06-14 21:49:00 INFO: Finished downloading models and saved to C:\\Users\\Dell\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy_entity_linker \"download_knowledge_base\"\n",
    "#!python -m spacy download en_core_web_lg\n",
    "#!python -m spacy download \"xx_ent_wiki_sm\"\n",
    "# !python -m spacy download es_core_news_md\n",
    "# !python -m spacy download pt_core_news_md\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import stanza\n",
    "import spacy_stanza\n",
    "\n",
    "stanza.download(\"ar\")\n",
    "stanza.download(\"vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: 154kB [00:00, 15.4MB/s]                    \n",
      "2022-06-14 22:43:21 WARNING: Language ar package default expects mwt, which has been added\n",
      "2022-06-14 22:43:23 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "| ner       | aqmar   |\n",
      "=======================\n",
      "\n",
      "2022-06-14 22:43:23 INFO: Use device: cpu\n",
      "2022-06-14 22:43:23 INFO: Loading: tokenize\n",
      "2022-06-14 22:43:23 INFO: Loading: mwt\n",
      "2022-06-14 22:43:23 INFO: Loading: ner\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nlp_en = spacy.load(\"en_core_web_lg\")\n",
    "nlp_es = spacy.load('es_core_news_md')\n",
    "nlp_pt = spacy.load(\"pt_core_news_md\")\n",
    "nlp_xx = spacy.load(\"xx_ent_wiki_sm\")\n",
    "\n",
    "nlp_ar = spacy_stanza.load_pipeline(\"ar\", processors='tokenize, ner')\n",
    "nlp_vi = spacy_stanza.load_pipeline(\"vi\", processors='tokenize, ner')\n",
    "for name in nlp_ar.pipe_names:\n",
    "    print(name)\n",
    "\n",
    "#nlp = stanza.Pipeline(lang='ar', processors='tokenize,ner')\n",
    "nlp_en.add_pipe(\"ner\", name=\"ner_es\", source=nlp_es)\n",
    "nlp_en.add_pipe(\"ner\", name=\"ner_pt\", source=nlp_pt)\n",
    "nlp_en.add_pipe(\"ner\", name=\"ner_xx\", source=nlp_xx)\n",
    "\n",
    "\n",
    "nlp_en.add_pipe(\"ner\", source=nlp_ar)\n",
    "nlp_en.add_pipe(\"ner\", source=nlp_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name                                             link  \\\n",
      "0              Wes W Parker         https://www.facebook.com/wes.parker.9277   \n",
      "1       Kristen Fitzpatrick  https://www.facebook.com/kristen.fitzpatrick.12   \n",
      "2  Jorene Monares Dela Cruz  https://www.facebook.com/jorene.monaresdelacruz   \n",
      "\n",
      "                                         profile_pic             role  \\\n",
      "0  https://scontent.ffcm1-2.fna.fbcdn.net/v/t1.18...  admin/moderator   \n",
      "1  https://scontent.ffcm1-1.fna.fbcdn.net/v/t39.3...  admin/moderator   \n",
      "2  https://scontent.ffcm1-2.fna.fbcdn.net/v/t1.64...              NaN   \n",
      "\n",
      "                       id         type         labels                joined  \\\n",
      "0  9arFVr8dkfuuDextavLKFH  UserAccount  [UserAccount]                   NaN   \n",
      "1  mjHRdrVWvSJ2onFjyR65wy  UserAccount  [UserAccount]                   NaN   \n",
      "2  K9vNtK9UcY6bmkNtgkG93u  UserAccount  [UserAccount]  Joined last Thursday   \n",
      "\n",
      "             description members  \n",
      "0                    NaN     NaN  \n",
      "1                    NaN     NaN  \n",
      "2  Works at Facebook App     NaN  \n"
     ]
    }
   ],
   "source": [
    "with open(\"network.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = json.loads(f.read())\n",
    "    fb_df = pd.DataFrame(content.get(\"nodes\"))\n",
    "\n",
    "print(fb_df.head(3))\n",
    "#fb_df[\"location\"] = [None for _ in range(fb_df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex #important note: I'm choosing this library instead of the standard re library because some of the description values that follow the \n",
    "#\"Works at\" pattern have 2 or more whitespace characters between the 'at' and the next word. Dealing with this requires making the positive \n",
    "#lookaround variable-length, which is not supported in the default Python regex engine. Hence the different engine library choice.\n",
    "from numpy import nan\n",
    "\n",
    "#works_at_pattern = [{\"LOWER\": \"at\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "regex_pattern = r\"\\b(?<=\\bat\\s{1,2}?\\X{0,1})\\p{L}.*$\"\n",
    "#matcher.add(\"WorksAt\", [works_at_pattern])\n",
    "\n",
    "def apply_and_concat(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)\n",
    "    \n",
    "def process_doc_deps(doc: spacy.tokens.Doc):\n",
    "    if doc is nan or doc is None:\n",
    "        return doc\n",
    "    if (doc[0] is nan or doc[0] is None):\n",
    "        return doc[0]\n",
    "    result = ''\n",
    "    if (doc[0].pos_ in ['NOUN', 'ADJ', 'VERB']):\n",
    "        if doc[0].pos_ == 'VERB':\n",
    "            if doc[0].tag_ == 'VBZ':              \n",
    "                result = 'The subject ' + ''.join([(token.text.lower() if token == doc[0] else token.text) + token.whitespace_ for token in doc])\n",
    "            else:\n",
    "                result = 'The subject does ' + ''.join([(token.text.lower() if token == test_result[0] else token.text) + token.whitespace_ for token in doc])\n",
    "        else:\n",
    "            result = 'The subject is ' + ''.join([(token.text.lower() if token == test_result[0] else token.text) + token.whitespace_ for token in doc])\n",
    "    else:\n",
    "        result = doc.text\n",
    "    return result\n",
    "\n",
    "def get_entities(descr: str):\n",
    "    spacy_results = nlp_en(str(descr), disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "    location_results = []\n",
    "    occupation_results = []\n",
    "    misc_results = []\n",
    "    # works_at_rule_results = regex.search(regex_pattern, spacy_results.text, flags=regex.UNICODE)\n",
    "    # if (works_at_rule_results is not None):\n",
    "    #     start, end = works_at_rule_results.span()\n",
    "    #     span = spacy_results.char_span(start, end)\n",
    "    #     # This is a Span object or None if match doesn't map to valid token sequence\n",
    "    #     if span is not None:\n",
    "    #         location_results.append(span.text)\n",
    "    #     elif bool('\\u200E' in spacy_results.text):\n",
    "    #         location_results.append(works_at_rule_results.group())\n",
    "    for ent in spacy_results.ents:\n",
    "        if ent.label_ in ['LOC', 'ORG', 'GPE'] and ent.text not in location_results:\n",
    "            location_results.append(ent.text)\n",
    "        elif ent.label_ in ['FAC', 'PRODUCT'] and ent.text not in occupation_results:\n",
    "            occupation_results.append(ent.text)\n",
    "        elif ent.label_ in ['PERSON', 'PER', 'MISC']:\n",
    "            misc_results.append(ent.text)\n",
    "    results_tuple = ((', '.join(location_results) if len(location_results) != 0 else nan), (', '.join(occupation_results) if len(occupation_results) != 0 else nan), (', '.join(misc_results) if len(misc_results) != 0 else nan))\n",
    "    return results_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E866] Expected a string or 'Doc' as input, but got: <class 'float'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Dell\\Documents\\Personal\\Self-study & career\\Analysis Practice\\Thedcpatriots & Patrick Lancaster\\data-processing-enrichment.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dell/Documents/Personal/Self-study%20%26%20career/Analysis%20Practice/Thedcpatriots%20%26%20Patrick%20Lancaster/data-processing-enrichment.ipynb#ch0000015?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m \u001b[39mimport\u001b[39;00m displacy\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Dell/Documents/Personal/Self-study%20%26%20career/Analysis%20Practice/Thedcpatriots%20%26%20Patrick%20Lancaster/data-processing-enrichment.ipynb#ch0000015?line=2'>3</a>\u001b[0m test_result \u001b[39m=\u001b[39m nlp_en(nan)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dell/Documents/Personal/Self-study%20%26%20career/Analysis%20Practice/Thedcpatriots%20%26%20Patrick%20Lancaster/data-processing-enrichment.ipynb#ch0000015?line=3'>4</a>\u001b[0m new_result \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSubject does \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([(token\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mlower() \u001b[39mif\u001b[39;00m token \u001b[39m==\u001b[39m test_result[\u001b[39m0\u001b[39m] \u001b[39melse\u001b[39;00m token\u001b[39m.\u001b[39mtext) \u001b[39m+\u001b[39m token\u001b[39m.\u001b[39mwhitespace_ \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m test_result])\n",
      "File \u001b[1;32mc:\\Users\\Dell\\miniconda3\\lib\\site-packages\\spacy\\language.py:1005\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[0;32m    985\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    986\u001b[0m     text: Union[\u001b[39mstr\u001b[39m, Doc],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    989\u001b[0m     component_cfg: Optional[Dict[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Any]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    990\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Doc:\n\u001b[0;32m    991\u001b[0m     \u001b[39m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[39m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[39m    is preserved.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[39m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1005\u001b[0m     doc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_doc(text)\n\u001b[0;32m   1006\u001b[0m     \u001b[39mif\u001b[39;00m component_cfg \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1007\u001b[0m         component_cfg \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Dell\\miniconda3\\lib\\site-packages\\spacy\\language.py:1096\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(doc_like, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1095\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_doc(doc_like)\n\u001b[1;32m-> 1096\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE866\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(doc_like)))\n",
      "\u001b[1;31mValueError\u001b[0m: [E866] Expected a string or 'Doc' as input, but got: <class 'float'>."
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "test_result = nlp_en(nan)\n",
    "new_result = 'Subject does ' + ''.join([(token.text.lower() if token == test_result[0] else token.text) + token.whitespace_ for token in test_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr_doc = fb_df['description'].apply(lambda s: nlp_en(s, disable=['ner', 'lemmatizer', 'senter']) if s is not nan and s is not None else s)\n",
    "fb_df['description_normalized'] = descr_doc.apply(process_doc_deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                    NaN\n",
      "1                                    NaN\n",
      "2                  Works at Facebook App\n",
      "3                                   None\n",
      "4    Driver at Valley Golf Cainta, Rizal\n",
      "Name: description, dtype: object\n",
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "print(fb_df[:5]['description'])\n",
    "print(type(fb_df.loc[3, 'description']))\n",
    "del fb_df['description_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Works work VERB VBZ ROOT Xxxxx True False\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.token.Token' object has no attribute 'se'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Dell\\Documents\\Personal\\Self-study & career\\Analysis Practice\\Thedcpatriots & Patrick Lancaster\\data-processing-enrichment.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dell/Documents/Personal/Self-study%20%26%20career/Analysis%20Practice/Thedcpatriots%20%26%20Patrick%20Lancaster/data-processing-enrichment.ipynb#ch0000017?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m test2:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dell/Documents/Personal/Self-study%20%26%20career/Analysis%20Practice/Thedcpatriots%20%26%20Patrick%20Lancaster/data-processing-enrichment.ipynb#ch0000017?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(token\u001b[39m.\u001b[39mtext, token\u001b[39m.\u001b[39mlemma_, token\u001b[39m.\u001b[39mpos_, token\u001b[39m.\u001b[39mtag_, token\u001b[39m.\u001b[39mdep_,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dell/Documents/Personal/Self-study%20%26%20career/Analysis%20Practice/Thedcpatriots%20%26%20Patrick%20Lancaster/data-processing-enrichment.ipynb#ch0000017?line=8'>9</a>\u001b[0m             token\u001b[39m.\u001b[39mshape_, token\u001b[39m.\u001b[39mis_alpha, token\u001b[39m.\u001b[39mis_stop)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Dell/Documents/Personal/Self-study%20%26%20career/Analysis%20Practice/Thedcpatriots%20%26%20Patrick%20Lancaster/data-processing-enrichment.ipynb#ch0000017?line=9'>10</a>\u001b[0m     token\u001b[39m.\u001b[39;49mse\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.token.Token' object has no attribute 'se'"
     ]
    }
   ],
   "source": [
    "del fb_df['location']\n",
    "del fb_df['occupation']\n",
    "del fb_df['misc_ents']\n",
    "\n",
    "test2 = nlp_en('Works at Electrical and electronics engineering')\n",
    "spacy.explain('VBN')\n",
    "for token in test2:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)\n",
    "    token.se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_df = apply_and_concat(fb_df, 'description_normalized', get_entities, ['location', 'occupation', 'misc_ents']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name  \\\n",
      "8              Rambo Buteng   \n",
      "9            Nguyen Le Ngan   \n",
      "11             Ta Hoang Son   \n",
      "14                Bocboc Jl   \n",
      "21      Missy Deleon Sabado   \n",
      "...                     ...   \n",
      "11801         Alice Stevens   \n",
      "11806           Mike Lucker   \n",
      "11808         Eldred Halsey   \n",
      "11817  Debbie Lemons Newton   \n",
      "11819       Richard O'Neill   \n",
      "\n",
      "                                                    link  \\\n",
      "8                  https://www.facebook.com/rambo.boteng   \n",
      "9      https://www.facebook.com/profile.php?id=100077...   \n",
      "11     https://www.facebook.com/profile.php?id=100077...   \n",
      "14                    https://www.facebook.com/bocboc.jl   \n",
      "21                    https://www.facebook.com/isayyysbd   \n",
      "...                                                  ...   \n",
      "11801          https://www.facebook.com/alice.stevens.33   \n",
      "11806          https://www.facebook.com/mike.lucker.3344   \n",
      "11808  https://www.facebook.com/profile.php?id=100018...   \n",
      "11817                 https://www.facebook.com/deborahln   \n",
      "11819        https://www.facebook.com/richard.oneill.100   \n",
      "\n",
      "                                             profile_pic role  \\\n",
      "8      https://scontent.ffcm1-2.fna.fbcdn.net/v/t1.64...  NaN   \n",
      "9      https://scontent.ffcm1-2.fna.fbcdn.net/v/t39.3...  NaN   \n",
      "11     https://scontent.ffcm1-2.fna.fbcdn.net/v/t39.3...  NaN   \n",
      "14     https://scontent.ffcm1-1.fna.fbcdn.net/v/t1.64...  NaN   \n",
      "21     https://scontent.ffcm1-2.fna.fbcdn.net/v/t1.64...  NaN   \n",
      "...                                                  ...  ...   \n",
      "11801  https://scontent.ffcm1-2.fna.fbcdn.net/v/t1.64...  NaN   \n",
      "11806  https://scontent.ffcm1-2.fna.fbcdn.net/v/t1.18...  NaN   \n",
      "11808  https://scontent.ffcm1-1.fna.fbcdn.net/v/t1.64...  NaN   \n",
      "11817  https://scontent.ffcm1-1.fna.fbcdn.net/v/t39.3...  NaN   \n",
      "11819  https://scontent.ffcm1-2.fna.fbcdn.net/v/t1.18...  NaN   \n",
      "\n",
      "                           id         type         labels  \\\n",
      "8      baofYjPRXFQBuDU6ETTuxB  UserAccount  [UserAccount]   \n",
      "9      jvAJNc9DZZMEB442NxQeVf  UserAccount  [UserAccount]   \n",
      "11     PBsxMvYJy7RXzae6ERh6LB  UserAccount  [UserAccount]   \n",
      "14     E8KNXjJpzvgGRywPBqZpQn  UserAccount  [UserAccount]   \n",
      "21     Qwj4hX9zuXPTfh9jSf4c8N  UserAccount  [UserAccount]   \n",
      "...                       ...          ...            ...   \n",
      "11801  eojt3XZWof5gLT4wotofph  UserAccount  [UserAccount]   \n",
      "11806  m58rVGEmQAuhExaLpbY4fB  UserAccount  [UserAccount]   \n",
      "11808  LP4pD85avncZymRjQT7hsx  UserAccount  [UserAccount]   \n",
      "11817  JfveiTpTifDt2PmJtUhNyH  UserAccount  [UserAccount]   \n",
      "11819  nWcsVDNA5QiBbLQG8uocN7  UserAccount  [UserAccount]   \n",
      "\n",
      "                         joined                        description members  \\\n",
      "8       Joined about a week ago             Works at Self-Employed     NaN   \n",
      "9       Joined about a week ago            Tr∆∞·ªùng ƒê·∫°i H·ªçc Tr√† Vinh     NaN   \n",
      "11      Joined about a week ago    Works at J. Riley Williams, PLC     NaN   \n",
      "14      Joined about a week ago           Works at The Krusty Krab     NaN   \n",
      "21      Joined about a week ago           Works at The Krusty Krab     NaN   \n",
      "...                         ...                                ...     ...   \n",
      "11801  Joined about 2 years ago                               None     NaN   \n",
      "11806  Joined about 2 years ago                   Works at Retired     NaN   \n",
      "11808  Joined about 2 years ago  Works at Fairway Valley Golf Club     NaN   \n",
      "11817  Joined about 2 years ago                 Thomas Walker High     NaN   \n",
      "11819  Joined about 2 years ago      Clover Park Technical College     NaN   \n",
      "\n",
      "                              description_normalized location  \\\n",
      "8                 The subject works at Self-Employed      NaN   \n",
      "9                            Tr∆∞·ªùng ƒê·∫°i H·ªçc Tr√† Vinh      NaN   \n",
      "11       The subject works at J. Riley Williams, PLC      NaN   \n",
      "14              The subject works at The Krusty Krab      NaN   \n",
      "21              The subject works at The Krusty Krab      NaN   \n",
      "...                                              ...      ...   \n",
      "11801                            The subject is None      NaN   \n",
      "11806                   The subject works at Retired      NaN   \n",
      "11808  The subject works at Fairway Valley Golf Club      NaN   \n",
      "11817                             Thomas Walker High      NaN   \n",
      "11819                  Clover Park Technical College      NaN   \n",
      "\n",
      "                     occupation                             misc_ents  \n",
      "8                           NaN    The subject works at Self-Employed  \n",
      "9                           NaN                            Tr∆∞·ªùng ƒê·∫°i  \n",
      "11                          NaN  The subject works, J. Riley Williams  \n",
      "14                          NaN  The subject works at The Krusty Krab  \n",
      "21                          NaN  The subject works at The Krusty Krab  \n",
      "...                         ...                                   ...  \n",
      "11801                       NaN                           The subject  \n",
      "11806                       NaN          The subject works at Retired  \n",
      "11808  Fairway Valley Golf Club                  The subject works at  \n",
      "11817                       NaN                         Thomas Walker  \n",
      "11819                       NaN                           Clover Park  \n",
      "\n",
      "[1185 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "locations_list = fb_df['location'].value_counts()\n",
    "occupations_list = fb_df['occupation'].value_counts()\n",
    "misc_list = fb_df['misc_ents'].value_counts()\n",
    "uncaptured_vals_list = fb_df[fb_df['location'].isna() & fb_df['description'].notna()]\n",
    "print(uncaptured_vals_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "193aa6218da7f76a59dc2bd6ea87d2b224c2417ad29cdf12ab4b4772313a8948"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
