{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>The purpose of this notebook is to perform necessary cleaning, (pre-)processing, and enrichment of data collected from external OSIF sources, such as TweetBeaver and Facebook. </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><strong>Tweet Processing: @PLNewsToday timeline</strong></h5>\n",
    "\n",
    "The columns in the file `PLnewstoday timeline.csv` are `Tweet author`,`Tweet ID`,`Date posted`,`Tweet text`,`URL`,`Retweets`,`Favorited`,`Source`. Notably in comparison to other files of tweets such as `thedcpatriot-lancaster-twitter1.json` from Twint, this timeline file (which I downloaded from TweetBeaver) does not contain columns for hashtags, user mentions, hyperlinks, tweet geo-tagged location, and several other useful properties of tweets. Let's extract those properties from the tweets in the timeline ourselves and save the output to a new file, which we will call `PLnewstoday timeline processed.csv`. \n",
    "\n",
    "In addition to the standard tweet properties of interest like mentions, URLs, and hashtags, there are also some aspects of these tweets that are of special value to us due to the nature of their source (a journalist reporting from a war zone, with several of their stories deemed false or deceptive by outside experts). As such, I will use a few APIs on these tweets for additional feature extraction: the GATE Journalist Safety Analyzer, the GATE Rumour veracity classifier, and the GATE TwitIE Named Entity Recognizer for Tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install twitter-text-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from ttp import ttp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tweet author                Tweet ID                     Date posted  \\\n",
      "0  PLnewstoday  ID 1522888466773721088  Sat May 07 10:37:59 +0000 2022   \n",
      "1  PLnewstoday  ID 1522851834431516672  Sat May 07 08:12:25 +0000 2022   \n",
      "2  PLnewstoday  ID 1522477719707172864  Fri May 06 07:25:49 +0000 2022   \n",
      "\n",
      "                                          Tweet text  \\\n",
      "0   RT @PLnewstoday: ‚ö°Ô∏èüì£Inside Azovstal Territory...   \n",
      "1   ‚ö°Ô∏èüì£Inside Azovstal Territory: First Western J...   \n",
      "2   ‚ö°Ô∏èüì£Ukraine Snipers Killed Civilians In Mariup...   \n",
      "\n",
      "                                                 URL  Retweets  Favorited  \\\n",
      "0  https://twitter.com/PLnewstoday/statuses/15228...       209          0   \n",
      "1  https://twitter.com/PLnewstoday/statuses/15228...       209        409   \n",
      "2  https://twitter.com/PLnewstoday/statuses/15224...       296        558   \n",
      "\n",
      "            Source  \n",
      "0  Twitter Web App  \n",
      "1  Twitter Web App  \n",
      "2  Twitter Web App  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/PLnewstoday timeline.csv\")\n",
    "print(df.head(3))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet ID'] = df['Tweet ID'].str.slice_replace(stop=3)\n",
    "df['Mentions'] = [[] for _ in range(df.shape[0])]\n",
    "df['Hashtags'] = [[] for _ in range(df.shape[0])]\n",
    "df['URLs'] = [[] for _ in range(df.shape[0])] #will have to be careful about this name - easy to mix up with the URL column from TweetBeaver\n",
    "df.rename(columns={\"Tweet text\": \"Text\", \"URL\": \"Link\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ttp.Parser()\n",
    "#df['Mentions'] = df.eval(parse_result.[0] if (parse_result:= parser.parse(df['Tweet text'])) else None\n",
    "df['parsed_results'] = df['Text'].apply(parser.parse)\n",
    "df['Mentions'] = df['parsed_results'].apply(lambda x: x.users)\n",
    "df['Hashtags'] = df['parsed_results'].apply(lambda y: y.tags)\n",
    "df['URLs'] = df['parsed_results'].apply(lambda z: z.urls)\n",
    "del df['parsed_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tweet author             Tweet ID                     Date posted  \\\n",
      "0  PLnewstoday  1522888466773721088  Sat May 07 10:37:59 +0000 2022   \n",
      "1  PLnewstoday  1522851834431516672  Sat May 07 08:12:25 +0000 2022   \n",
      "2  PLnewstoday  1522477719707172864  Fri May 06 07:25:49 +0000 2022   \n",
      "\n",
      "                                                Text  \\\n",
      "0   RT @PLnewstoday: ‚ö°Ô∏èüì£Inside Azovstal Territory...   \n",
      "1   ‚ö°Ô∏èüì£Inside Azovstal Territory: First Western J...   \n",
      "2   ‚ö°Ô∏èüì£Ukraine Snipers Killed Civilians In Mariup...   \n",
      "\n",
      "                                                Link  Retweets  Favorited  \\\n",
      "0  https://twitter.com/PLnewstoday/statuses/15228...       209          0   \n",
      "1  https://twitter.com/PLnewstoday/statuses/15228...       209        409   \n",
      "2  https://twitter.com/PLnewstoday/statuses/15224...       296        558   \n",
      "\n",
      "            Source       Mentions            Hashtags  \\\n",
      "0  Twitter Web App  [PLnewstoday]                  []   \n",
      "1  Twitter Web App             []  [RussiaUkraineWar]   \n",
      "2  Twitter Web App             []                  []   \n",
      "\n",
      "                        URLs  \n",
      "0          [https://t.co/Sh]  \n",
      "1  [https://t.co/ShE3eRNN98]  \n",
      "2  [https://t.co/yPN16UFQP6]  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Facebook Profile Processing</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from stanza.pipeline.multilingual import MultilingualPipeline\n",
    "\n",
    "stanza.download(\"ar\")\n",
    "stanza.download(\"vi\")\n",
    "stanza.download(\"multilingual\")\n",
    "stanza.download(\"bg\")\n",
    "stanza.download(\"tr\")\n",
    "stanza.download(\"be\")\n",
    "stanza.download(\"he\")\n",
    "stanza.download(\"id\")\n",
    "stanza.download(\"ko\")\n",
    "stanza.download(\"es\")\n",
    "stanza.download(\"pt\")\n",
    "stanza.download(\"en\")\n",
    "stanza.download(\"en\", processors=\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-01 17:26:09 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615f76f2481c419cacd05f3d8379cafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-01 17:26:10 INFO: Loading these models for language: multilingual ():\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| langid    | ud      |\n",
      "=======================\n",
      "\n",
      "2023-01-01 17:26:10 INFO: Use device: cpu\n",
      "2023-01-01 17:26:10 INFO: Loading: langid\n",
      "2023-01-01 17:26:11 INFO: Done loading processors!\n",
      "2023-01-01 17:26:11 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b18edec38ff47a699fb5ced02fdbb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-01 17:26:11 INFO: Loading these models for language: multilingual ():\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| langid    | ud      |\n",
      "=======================\n",
      "\n",
      "2023-01-01 17:26:11 INFO: Use device: cpu\n",
      "2023-01-01 17:26:11 INFO: Loading: langid\n",
      "2023-01-01 17:26:11 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# nlp_en = spacy.load(\"en_core_web_lg\")\n",
    "# nlp_es = spacy.load('es_core_news_md')\n",
    "# nlp_pt = spacy.load(\"pt_core_news_md\")\n",
    "# nlp_xx = spacy.load(\"xx_ent_wiki_sm\")\n",
    "# nlp_zh = spacy.load(\"zh_core_web_sm\")\n",
    "# nlp_ko = spacy.load(\"ko_core_news_sm\")\n",
    "\n",
    "# nlp_ar = spacy_stanza.load_pipeline(\"ar\", processors='tokenize, ner')\n",
    "# nlp_vi = spacy_stanza.load_pipeline(\"vi\", processors='tokenize, ner')\n",
    "# nlp_bg = spacy_stanza.load_pipeline(\"bg\", processors='tokenize, ner')\n",
    "# nlp_tr = spacy_stanza.load_pipeline(\"tr\", processors='tokenize, ner')\n",
    "\n",
    "# #nlp = stanza.Pipeline(lang='ar', processors='tokenize,ner')\n",
    "# nlp_en.add_pipe(\"ner\", name=\"ner_es\", source=nlp_es)\n",
    "# nlp_en.add_pipe(\"ner\", name=\"ner_pt\", source=nlp_pt)\n",
    "# nlp_en.add_pipe(\"ner\", name=\"ner_xx\", source=nlp_xx)\n",
    "# nlp_en.add_pipe(\"ner\", name=\"ner_zh\", source=nlp_zh)\n",
    "# nlp_en.add_pipe(\"ner\", name=\"ner_ko\", source=nlp_ko)\n",
    "\n",
    "nlp_multi = MultilingualPipeline(lang_id_config={\n",
    "    \"langid_clean_text\": False, \n",
    "    \"langid_lang_subset\": [\"en\",\"ar\", \"es\", \"pt\", \"be\", \"bg\", \"ko\", \"id\", \"he\", \"ru\", \"th\", \"tr\", \"vi\" ],\n",
    "    }, \n",
    "    lang_configs={\n",
    "        \"en\": {\"processors\": 'tokenize, pos, ner', \"download_method\": stanza.DownloadMethod.REUSE_RESOURCES},\n",
    "        \"ar\": {\"processors\": 'tokenize, ner', \"download_method\": stanza.DownloadMethod.REUSE_RESOURCES},\n",
    "        \"es\": {\"processors\": 'tokenize, pos, ner', \"download_method\": stanza.DownloadMethod.REUSE_RESOURCES},\n",
    "        \"pt\": {\"processors\": 'tokenize, pos', \"download_method\": stanza.DownloadMethod.REUSE_RESOURCES},\n",
    "        \"be\": {\"processors\": 'tokenize', \"download_method\": stanza.DownloadMethod.REUSE_RESOURCES},\n",
    "        \"bg\": {\"processors\": 'tokenize, ner', \"download_method\": stanza.DownloadMethod.REUSE_RESOURCES},\n",
    "        \"he\": {\"processors\": 'tokenize', \"download_method\": stanza.DownloadMethod.REUSE_RESOURCES},\n",
    "        \"id\": {\"processors\": 'tokenize', \"download_method\": stanza.DownloadMethod.REUSE_RESOURCES},\n",
    "        \"ko\": {\"processors\": 'tokenize', \"download_method\": stanza.DownloadMethod.REUSE_RESOURCES},\n",
    "        \"th\": {\"processors\": 'tokenize', \"download_method\": stanza.DownloadMethod.REUSE_RESOURCES},\n",
    "        \"tr\": {\"processors\": 'tokenize, ner', \"download_method\": stanza.DownloadMethod.REUSE_RESOURCES},\n",
    "        \"vi\": {\"processors\": 'tokenize, ner', \"download_method\": stanza.DownloadMethod.REUSE_RESOURCES}\n",
    "    }, \n",
    "    ld_batch_size=85,\n",
    "    max_cache_size=15 )\n",
    "\n",
    "lang_detector = stanza.Pipeline(lang=\"multilingual\", processors=\"langid\", langid_lang_subset=[\"en\",\"ar\", \"es\", \"pt\", \"be\", \"bg\", \"ko\", \"id\", \"he\", \"ru\", \"tr\", \"vi\" ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data-temp/network.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = json.loads(f.read())\n",
    "    fb_df = pd.DataFrame(content.get(\"nodes\"))\n",
    "    \n",
    "fb_df.drop(columns=[\"members\"], inplace=True)\n",
    "print(fb_df.head(3))\n",
    "#fb_df[\"location\"] = [None for _ in range(fb_df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex #important note: I'm choosing this library instead of the standard re library because some of the description values that follow the \n",
    "#\"Works at\" pattern have 2 or more whitespace characters between the 'at' and the next word. Dealing with this requires making the positive \n",
    "#lookaround variable-length, which is not supported in the default Python regex engine. Hence the different engine library choice.\n",
    "from numpy import nan\n",
    "\n",
    "#works_at_pattern = [{\"LOWER\": \"at\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "regex_pattern = r\"\\b(?<=\\bat\\s{1,2}?\\X{0,1})\\p{L}.*$\"\n",
    "#matcher.add(\"WorksAt\", [works_at_pattern])\n",
    "\n",
    "def apply_and_concat(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)\n",
    "    \n",
    "def apply_and_concat_external(target_dataframe, source_series, func, column_names):\n",
    "    return pd.concat((\n",
    "        target_dataframe,\n",
    "        source_series.apply(lambda cell: pd.Series(func(cell), index=column_names))\n",
    "    ), axis=1)\n",
    "\n",
    "def get_lang(descr: str):\n",
    "    if descr is nan or descr is None:\n",
    "        return nan\n",
    "    doc = lang_detector(descr)\n",
    "    return doc.lang\n",
    "    \n",
    "# def process_doc_deps(doc: tokens.Doc):\n",
    "#     if doc is nan or doc is None:\n",
    "#         return doc\n",
    "#     if (doc[0] is nan or doc[0] is None):\n",
    "#         return doc[0]\n",
    "#     result = ''\n",
    "#     if (doc[0].pos_ in ['NOUN', 'ADJ', 'VERB']):\n",
    "#         if doc[0].pos_ == 'VERB':\n",
    "#             if doc[0].tag_ == 'VBZ':              \n",
    "#                 result = 'The subject ' + ''.join([(token.text.lower() if token == doc[0] else token.text) + token.whitespace_ for token in doc])\n",
    "#             else:\n",
    "#                 result = 'The subject does ' + ''.join([(token.text.lower() if token == doc[0] else token.text) + token.whitespace_ for token in doc])\n",
    "#         else:\n",
    "#             result = 'The subject is ' + ''.join([(token.text.lower() if token == doc[0] else token.text) + token.whitespace_ for token in doc])\n",
    "#     else:\n",
    "#         result = doc.text\n",
    "#     return result\n",
    "\n",
    "# def get_entities(descr: str):\n",
    "#     spacy_results = nlp_en(str(descr), disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "#     location_results = []\n",
    "#     occupation_results = []\n",
    "#     misc_results = []\n",
    "#     # works_at_rule_results = regex.search(regex_pattern, spacy_results.text, flags=regex.UNICODE)\n",
    "#     # if (works_at_rule_results is not None):\n",
    "#     #     start, end = works_at_rule_results.span()\n",
    "#     #     span = spacy_results.char_span(start, end)\n",
    "#     #     # This is a Span object or None if match doesn't map to valid token sequence\n",
    "#     #     if span is not None:\n",
    "#     #         location_results.append(span.text)\n",
    "#     #     elif bool('\\u200E' in spacy_results.text):\n",
    "#     #         location_results.append(works_at_rule_results.group())\n",
    "#     for ent in spacy_results.ents:\n",
    "#         if ent.label_ in ['LOC', 'ORG', 'GPE', 'LC', 'OG'] and ent.text not in location_results:\n",
    "#             location_results.append(ent.text)\n",
    "#         elif ent.label_ in ['FAC', 'PRODUCT'] and ent.text not in occupation_results:\n",
    "#             occupation_results.append(ent.text)\n",
    "#         elif ent.label_ in ['PERSON', 'PER', 'PS', 'MISC']:\n",
    "#             misc_results.append(ent.text)\n",
    "#     results_tuple = ((', '.join(location_results) if len(location_results) != 0 else nan), (', '.join(occupation_results) if len(occupation_results) != 0 else nan), (', '.join(misc_results) if len(misc_results) != 0 else nan))\n",
    "#     return results_tuple\n",
    "\n",
    "\n",
    "def get_lang_and_entities(doc: stanza.models.common.doc.Document):\n",
    "    location_results = []\n",
    "    occupation_results = []\n",
    "    misc_results = []\n",
    "    other_results = []\n",
    "    untagged_results = []\n",
    "    if len(doc.ents) > 0:\n",
    "        for ent in doc.ents:\n",
    "            if ent.type in ['LOC', 'ORG', 'GPE', 'LC', 'OG', 'LOCATION', 'ORGANIZATION', 'GPE']:\n",
    "                location_results.append(ent.text)\n",
    "            elif ent.type in ['FAC', 'PRODUCT']:\n",
    "                occupation_results.append(ent.text)\n",
    "            elif ent.type in ['PERSON', 'PER', 'PS', 'MISC', 'MISCELLANEOUS']:\n",
    "                misc_results.append(ent.text)\n",
    "            else:\n",
    "                other_results.append(ent.text)\n",
    "    else:\n",
    "        works_at_rule_results = regex.search(regex_pattern, doc.text, flags=regex.UNICODE)\n",
    "        if (works_at_rule_results is not None):\n",
    "            start, end = works_at_rule_results.span()\n",
    "            span = doc.text[start:end]\n",
    "            untagged_results.append(span)\n",
    "    results_tuple = (\n",
    "                     (doc.lang), \n",
    "                     (', '.join(location_results) if len(location_results) != 0 else nan), \n",
    "                     (', '.join(occupation_results) if len(occupation_results) != 0 else nan), \n",
    "                     (', '.join(misc_results) if len(misc_results) != 0 else nan),\n",
    "                     (', '.join(other_results) if len(other_results) != 0 else nan),\n",
    "                     (', '.join(untagged_results) if len(untagged_results) != 0 else nan)\n",
    "                    )\n",
    "    return results_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_series = fb_df['description'][fb_df['description'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-01 17:26:54 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-01-01 17:26:54 INFO: Use device: cpu\n",
      "2023-01-01 17:26:54 INFO: Loading: tokenize\n",
      "2023-01-01 17:26:54 INFO: Loading: pos\n",
      "2023-01-01 17:26:55 INFO: Loading: ner\n",
      "2023-01-01 17:26:57 INFO: Done loading processors!\n",
      "2023-01-01 17:34:32 INFO: Loading these models for language: vi (Vietnamese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | vtb     |\n",
      "| ner       | vlsp    |\n",
      "=======================\n",
      "\n",
      "2023-01-01 17:34:32 INFO: Use device: cpu\n",
      "2023-01-01 17:34:32 INFO: Loading: tokenize\n",
      "2023-01-01 17:34:32 INFO: Loading: ner\n",
      "2023-01-01 17:34:34 INFO: Done loading processors!\n",
      "2023-01-01 17:34:40 WARNING: Language pt package default expects mwt, which has been added\n",
      "2023-01-01 17:34:44 INFO: Loading these models for language: pt (Portuguese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | bosque  |\n",
      "| mwt       | bosque  |\n",
      "| pos       | bosque  |\n",
      "=======================\n",
      "\n",
      "2023-01-01 17:34:44 INFO: Use device: cpu\n",
      "2023-01-01 17:34:44 INFO: Loading: tokenize\n",
      "2023-01-01 17:34:44 INFO: Loading: mwt\n",
      "2023-01-01 17:34:44 INFO: Loading: pos\n",
      "2023-01-01 17:34:45 INFO: Done loading processors!\n",
      "2023-01-01 17:34:54 WARNING: Language id package default expects mwt, which has been added\n",
      "2023-01-01 17:34:54 INFO: Loading these models for language: id (Indonesian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "=======================\n",
      "\n",
      "2023-01-01 17:34:54 INFO: Use device: cpu\n",
      "2023-01-01 17:34:54 INFO: Loading: tokenize\n",
      "2023-01-01 17:34:54 INFO: Loading: mwt\n",
      "2023-01-01 17:34:54 INFO: Done loading processors!\n",
      "2023-01-01 17:35:07 WARNING: Language es package default expects mwt, which has been added\n",
      "2023-01-01 17:35:17 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2023-01-01 17:35:17 INFO: Use device: cpu\n",
      "2023-01-01 17:35:17 INFO: Loading: tokenize\n",
      "2023-01-01 17:35:17 INFO: Loading: mwt\n",
      "2023-01-01 17:35:18 INFO: Loading: pos\n",
      "2023-01-01 17:35:19 INFO: Loading: ner\n",
      "2023-01-01 17:35:21 INFO: Done loading processors!\n",
      "2023-01-01 17:35:44 WARNING: Language ar package default expects mwt, which has been added\n",
      "2023-01-01 17:35:50 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "| ner       | aqmar   |\n",
      "=======================\n",
      "\n",
      "2023-01-01 17:35:50 INFO: Use device: cpu\n",
      "2023-01-01 17:35:50 INFO: Loading: tokenize\n",
      "2023-01-01 17:35:50 INFO: Loading: mwt\n",
      "2023-01-01 17:35:50 INFO: Loading: ner\n",
      "2023-01-01 17:35:52 INFO: Done loading processors!\n",
      "2023-01-01 17:35:53 INFO: Loading these models for language: be (Belarusian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hse     |\n",
      "=======================\n",
      "\n",
      "2023-01-01 17:35:53 INFO: Use device: cpu\n",
      "2023-01-01 17:35:53 INFO: Loading: tokenize\n",
      "2023-01-01 17:35:53 INFO: Done loading processors!\n",
      "2023-01-01 17:35:53 INFO: Loading these models for language: ko (Korean):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | kaist   |\n",
      "=======================\n",
      "\n",
      "2023-01-01 17:35:53 INFO: Use device: cpu\n",
      "2023-01-01 17:35:53 INFO: Loading: tokenize\n",
      "2023-01-01 17:35:53 INFO: Done loading processors!\n",
      "2023-01-01 17:35:53 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da418191f2ed4e45aeb84113d3fbbf5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-01 17:36:11 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "| ner       | wikiner   |\n",
      "=========================\n",
      "\n",
      "2023-01-01 17:36:11 INFO: Use device: cpu\n",
      "2023-01-01 17:36:11 INFO: Loading: tokenize\n",
      "2023-01-01 17:36:11 INFO: Loading: pos\n",
      "2023-01-01 17:36:12 INFO: Loading: lemma\n",
      "2023-01-01 17:36:12 INFO: Loading: depparse\n",
      "2023-01-01 17:36:13 INFO: Loading: ner\n",
      "2023-01-01 17:36:16 INFO: Done loading processors!\n",
      "2023-01-01 17:36:23 INFO: Loading these models for language: bg (Bulgarian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | btb     |\n",
      "| ner       | bsnlp19 |\n",
      "=======================\n",
      "\n",
      "2023-01-01 17:36:23 INFO: Use device: cpu\n",
      "2023-01-01 17:36:23 INFO: Loading: tokenize\n",
      "2023-01-01 17:36:23 INFO: Loading: ner\n",
      "2023-01-01 17:36:25 INFO: Done loading processors!\n",
      "2023-01-01 17:36:26 WARNING: Language tr package default expects mwt, which has been added\n",
      "2023-01-01 17:36:31 INFO: Loading these models for language: tr (Turkish):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | imst     |\n",
      "| mwt       | imst     |\n",
      "| ner       | starlang |\n",
      "========================\n",
      "\n",
      "2023-01-01 17:36:31 INFO: Use device: cpu\n",
      "2023-01-01 17:36:31 INFO: Loading: tokenize\n",
      "2023-01-01 17:36:31 INFO: Loading: mwt\n",
      "2023-01-01 17:36:31 INFO: Loading: ner\n",
      "2023-01-01 17:36:33 INFO: Done loading processors!\n",
      "2023-01-01 17:36:34 WARNING: Language he package default expects mwt, which has been added\n",
      "2023-01-01 17:36:34 INFO: Loading these models for language: he (Hebrew):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "2023-01-01 17:36:34 INFO: Use device: cpu\n",
      "2023-01-01 17:36:34 INFO: Loading: tokenize\n",
      "2023-01-01 17:36:34 INFO: Loading: mwt\n",
      "2023-01-01 17:36:34 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "print(not isinstance(fb_df['description'], list))\n",
    "\n",
    "docs_list = docs_series.to_list()\n",
    "\n",
    "\n",
    "langed_docs = nlp_multi(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(langed_docs[0]))\n",
    "for doc in langed_docs:\n",
    "    print(\"---\")\n",
    "    print(f\"text: {doc.text}\")\n",
    "    print(f\"lang: {doc.lang}\")\n",
    "    print(f\"ents: {doc.ents}\")\n",
    "    print(f\"{doc.sentences[0].dependencies_string()}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "langed_series = pd.Series(langed_docs, index=docs_series.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs_series = apply_and_concat_external(docs_series, langed_series, get_lang_and_entities, ['lang', 'location', 'occupation', 'misc_ents', 'other_ents', 'untagged']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n"
     ]
    }
   ],
   "source": [
    "res = nlp_multi(\"‡∏Ñ‡∏∑‡∏≠‡∏™‡∏á‡∏™‡∏±‡∏¢‡∏°‡∏≤‡∏ô‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡πâ‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ü‡∏ã‡∏ß‡πà‡∏≤ ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà ‡∏û‡πà‡∏≠‡∏Å‡∏±‡∏ö‡πÅ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏°‡∏´‡∏≤‡∏ä‡∏ô ‡∏Ñ‡∏∑‡∏≠‡∏°‡∏±‡∏ô‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ô‡∏µ‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÜ\")\n",
    "print(res.lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_df = fb_df.join(docs_series.loc[:, ['lang', 'location', 'occupation', 'misc_ents', 'other_ents', 'untagged']], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_df.to_csv(\"data-temp/fb_data_langid_semiprocessed.csv\", sep='\\t', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name           6921\n",
      "link           6921\n",
      "profile_pic    6921\n",
      "role              0\n",
      "id             6921\n",
      "type           6921\n",
      "labels         6921\n",
      "joined         6921\n",
      "description    6921\n",
      "members           0\n",
      "lang           6921\n",
      "location       3359\n",
      "occupation      194\n",
      "misc_ents       507\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "locations_list = fb_df['location'].value_counts()\n",
    "occupations_list = fb_df['occupation'].value_counts()\n",
    "misc_list = fb_df['misc_ents'].value_counts()\n",
    "uncaptured_vals_list = fb_df[(fb_df['location'].isna() | fb_df['occupation'].isna() | fb_df['misc_ents'].isna()) & fb_df['lang'].notna()]\n",
    "print(uncaptured_vals_list.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('arcgis_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 [MSC v.1927 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06978df5d7fdf2da75bbc77aec524566b5da5f95a69bdbf48b0bb60b6240550b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
